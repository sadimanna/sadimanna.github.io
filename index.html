<!DOCTYPE HTML>
<html lang="en"><head>
      <meta name="google-site-verification" content="mzyi5h_x1DvmnvvOynztP5h3mA2mdBpUjfhySMB9NNI" />
      <!-- Google Tag Manager -->
      <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-NS48J6H');</script>
      <!-- End Google Tag Manager -->

      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <title>Siladittya Manna</title>
      <meta name="author" content="Siladittya Manna">
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <link rel="stylesheet" type="text/css" href="style.css">
      <link rel="stylesheet" type="text/css" href="stylesheet.css">
      <!-- <link rel="stylesheet" href="fontawesome.all.min.css"> -->
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
<!--       <link rel="icon" type="image/png" href="images/icon.png"> -->
      <link rel="icon" href="assets/profile_circle.png">
      <!-- <script defer src="./static/js/fontawesome.all.min.js"></script> -->


      <!-- Global site tag (gtag.js) - Google Analytics -->
      <!-- Need to change ############## -->
      <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=GTM-NS48J6H"></script>
      <script>
         window.dataLayer = window.dataLayer || [];
         function gtag(){dataLayer.push(arguments);}
         gtag('js', new Date());
         
         gtag('config', 'GTM-NS48J6H');
      </script> -->
      <!-- Need to change ############## -->
   </head>

<body>
  <table style="width:100%;max-width:1200px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Siladittya Manna</name>
              </p>
              <p>I am a Senior Research Fellow at <a href="https://www.isical.ac.in/">Computer Vision and Pattern Recognition Unit</a>, Indian Statistical Institute, Kolkata. My area of research includes Self-supervised learning, computer vision and medical image analysis.
              </p>
              <p style="text-align:center">
                <a href="mailto:mannasiladittya@gmail.com"><img src="assets/envelope-solid.svg" alt="Icon" class="icon">Email</a> <br> <!--&nbsp/&nbsp-->
                <a href="assets/Siladittya Manna Curriculum Vitae.pdf"><img src="assets/file-solid.svg" alt="Icon" class="icon">CV</a> &nbsp|&nbsp
		      <a href="https://github.com/sadimanna/"><img src="assets/github.svg" alt="Icon" class="icon">Github</a> <br>
                <a href="https://scholar.google.com/citations?hl=en&user=6V9sqi0AAAAJ"><img src="assets/google-scholar.svg" alt="Icon" class="icon">Google Scholar</a> &nbsp|&nbsp
		<a href="https://dblp.org/pid/270/2011.html"><img src="assets/dblp.svg" alt="Icon" class="icon">dblp</a> &nbsp|&nbsp
		<a href="https://orcid.org/0000-0001-6364-8654"><img src="assets/orcid.svg" alt="Icon" class="icon">Orcid</a> <br>
		<a href="https://twitter.com/sadimanna"><img src="assets/twitter.svg" alt="Icon" class="icon">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="assets/profilepic.png"><img style="width:100%;max-width:100%" alt="profile photo" src="assets/profilepic.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Bio</heading>
              <p>
                I am pursuing my PhD at Indian Statistical Institute, Kolkata, where I am advised by <a href="https://www.isical.ac.in/~umapada/">Prof. Umapada Pal</a> and <a href="https://www.iitkgp.ac.in/department/EC/faculty/ec-saumik">Dr. Saumik Bhattacharya</a>. Before joining ISI, Kolkata, I did my B.Tech and M.Tech (Dual Degree) from Indian Institute of Engineering Science and Technology, Shibpur, Howrah, where I was advised by <a href="https://www.iiests.ac.in/IIEST/Faculty/telecom-ankita">Dr. Ankita Pramanik</a> for my Master's Thesis.
              </p>
            </td>
          </tr>
        </tbody></table>
	
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="padding:20px;width:100%;vertical-align:middle">
                              <heading>
                                 <font size="5">Recent Updates </font>
                              </heading>
                              <p>
				<li>
                                 <a class="button" style="color:#FFFFFF"><strong style="font-size:12px"> New! </strong></a>
                                 <em style="font-size:14px;"><b>[July 2023:]</b> <a href="https://arxiv.org/abs/2104.10481"><strong>One paper</strong></a> accepted to <strong>IEEE Transactions on Artificial Intelligence</strong>.</em>
                              </li>
                              <li>
                                 <a class="button" style="color:#FFFFFF"><strong style="font-size:12px"> New! </strong></a>
                                 <em style="font-size:14px;"><b>[Apr 2023:]</b> <a href="https://arxiv.org/abs/2305.00795"><strong>One paper</a></strong> accepted in <a href="https://icdar2023.org/"> <strong>ICDAR 2023</strong> for <strong>Oral</strong> presentation</a>.</em>
                              </li>
<!--                               <li>
                                 <em style="font-size:14px;"><b>[Dec 2017:]</b> <a href="https://ieeexplore.ieee.org/abstract/document/8593100/"> One paper</a> accepted in <a href="https://www.isical.ac.in/~icapr17/">ICAPR 2017</a>.</em>
                              </li> -->
                              </p>
                           </td>
                        </tr>
                     </tbody>
        </table>
		
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>2023</heading>
            </td>
          </tr>
        </tbody></table>
        
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
<!--           <div class="two" id='swis_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='assets/dystress_image.png' width="180">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2308.01140">
          <papertitle>DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning</papertitle>
        </a>
        <br>
        <strong>Siladittya Manna</strong>,
	      <a href="https://soumitri2001.github.io/">Soumitri Chattopadhyay</a>,
	      <a href="https://scholar.google.com/citations?hl=en&user=sz1Hu3gAAAAJ">Rakesh Dey</a>,
	      <a href="https://saumikb.github.io/">Saumik Bhattacharya</a>,
        <a href="https://www.isical.ac.in/~umapada/">Umapada Pal</a>,<br>
        <em>Under Review</em>
        <br>
<!--         <a href="https://ieeexplore.ieee.org/abstract/document/9897562/"><span><i class="fas fa-scroll"></i></span>Paper</a>
        / -->
<!-- 	<a href="https://soumitri2001.github.io/assets/SWIS_PPT.pdf"><span><i class="fa-solid fa-presentation-screen"></i></span>Slides</a>
	/ -->
        <a href="https://arxiv.org/abs/2308.01140"><span><i class="ai ai-arxiv"></i></span>arXiv</a>
        <p></p>
        <p>
       We focus our attention to improve the performance of InfoNCE loss in SSL by studying the effect of temperature 
		hyper-parameter values. We propose a cosine similarity-dependent temperature scaling function to effectively optimize the 
		distribution of the samples in the feature space. We further analyze the uniformity and tolerance metrics to investigate 
		the optimal regions in the cosine similarity space for better optimization. Additionally, we offer a comprehensive examination 
		of the behavior of local and global structures in the feature space throughout the pre-training phase, as the temperature varies.
        </p>
      </td>
    </tr>

   <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
<!--           <div class="two" id='swis_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='assets/skid_image.png' width="180">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://doi.org/10.1109/TAI.2023.3299883">
          <papertitle>Self-Supervised Representation Learning for Knee Injury Diagnosis from Magnetic Resonance Data</papertitle>
        </a>
        <br>
        <strong>Siladittya Manna</strong>,
        <a href="https://saumikb.github.io/">Saumik Bhattacharya</a>,
        <a href="https://www.isical.ac.in/~umapada/">Umapada Pal</a>,  <br>
        <em>IEEE Transactions on Aritifical Intelligence</em>
        <br>
        <a href="https://ieeexplore.ieee.org/abstract/document/10197577"><span><i class="fas fa-scroll"></i></span>Paper</a>
        /
	<a href="https://github.com/sadimanna/skid"><span><i class="fa-solid fa-code"></i></span>Code</a>
	/
        <a href="https://arxiv.org/abs/2104.10481"><span><i class="ai ai-arxiv"></i></span>arXiv</a>
        <p></p>
        <p>
        We propose a self-supervised learning (SSL) approach for learning the spatial anatomical representations from the frames of magnetic resonance (MR) video clips for the diagnosis of knee medical conditions. The pretext model learns meaningful, context-invariant spatial representations. The downstream task in our paper is a class-imbalanced multi-label classification. To the best of our knowledge, this work is the first of its kind in showing the effectiveness and reliability of self-supervised learning algorithms in imbalanced multi-label classification tasks on MR scans.
        </p>
      </td>
    </tr>
  
<tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
<!--           <div class="two" id='swis_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='assets/mio_image.png' width="180">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2111.12664">
          <papertitle>MIO: Mutual Information Optimization using Self-Supervised Binary Contrastive Learning</papertitle>
        </a>
        <br>
        <strong>Siladittya Manna</strong>,
        <a href="https://www.isical.ac.in/~umapada/">Umapada Pal</a>,
	      <a href="https://saumikb.github.io/">Saumik Bhattacharya</a>,<br>
        <em>Under Review</em>
        <br>
<!--         <a href="https://ieeexplore.ieee.org/abstract/document/9897562/"><span><i class="fas fa-scroll"></i></span>Paper</a>
        / -->
<!-- 	<a href="https://soumitri2001.github.io/assets/SWIS_PPT.pdf"><span><i class="fa-solid fa-presentation-screen"></i></span>Slides</a>
	/ -->
        <a href="https://arxiv.org/abs/2111.12664"><span><i class="ai ai-arxiv"></i></span>arXiv</a>
        <p></p>
        <p>
        We propose a novel mutual information optimization-based loss function for contrastive learning. We model our pre-training task as a binary
	classification problem to induce an implicit contrastive effect and predict whether a pair is positive or negative. We further improve the
	naive loss function using the Majorize-Minimizer principle and such improvement helps us to track the problem mathematically. Unlike
	the existing methods, the proposed loss function optimizes the mutual information in both positive and negative pairs. We also present
	a closed-form expression for the parameter gradient flow and compare the behavior of the proposed loss function using its Hessian
	eigen-spectrum to analytically study the convergence of SSL frameworks. The proposed method outperforms the SOTA contrastive
	self-supervised frameworks on benchmark datasets like CIFAR-10, CIFAR-100, STL-10, and Tiny-ImageNet. After 200 epochs of
	pre-training with ResNet-18 as the backbone, the proposed model achieves an accuracy of 86.2%, 58.18%, 77.49%, and 30.87% on
	CIFAR-10, CIFAR-100, STL-10, and Tiny-ImageNet datasets, respectively, and surpasses the SOTA contrastive baseline by 1.23%,
	3.57%, 2.00%, and 0.33%, respectively.
        </p>
      </td>
    </tr>
		
   <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
<!--           <div class="two" id='swis_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='assets/selfdocseg.png' width="180">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2305.00795">
          <papertitle>SelfDocSeg: A Self-Supervised vision-based Approach towards Document Segmentation</papertitle>
        </a>
        <br>
	      <a href="">Subhajit Maity</a>,
	      <a href="">Sanket Biswas</a>,
        <strong>Siladittya Manna</strong>,
        <a href="">Ayan Banerjee</a>,
	      <a href="">Joseph Llados</a>,
        <a href="https://saumikb.github.io/">Saumik Bhattacharya</a>,
        <a href="https://www.isical.ac.in/~umapada/">Umapada Pal</a>,  <br>
        <em>18th International Conference on Document Analysis and Recognition (ICDAR)</em>, 2024 (Oral)
        <br>
<!--         <a href="https://ieeexplore.ieee.org/abstract/document/9897562/"><span><i class="fas fa-scroll"></i></span>Paper</a>
        / -->
<!-- 	<a href="https://soumitri2001.github.io/assets/SWIS_PPT.pdf"><span><i class="fa-solid fa-presentation-screen"></i></span>Slides</a>
	/ -->
        <a href="https://arxiv.org/abs/2305.00795"><span><i class="ai ai-arxiv"></i></span>arXiv</a>
        <p></p>
        <p>
        With growing internet connectivity to personal life, an enormous amount of documents had been available in the public domain and thus making data annotation a tedious task. We address this challenge using self-supervision and unlike, the few existing self-supervised document segmentation approaches which use text mining and textual labels, we use a complete vision-based approach in pre-training without any ground-truth label or its derivative. Instead, we generate pseudo-layouts from the document images to pre-train an image encoder to learn the document object representation and localization in a self-supervised framework before fine-tuning it with an object detection model. We show that our pipeline sets a new benchmark in this context and performs at par with the existing methods and the supervised counterparts, if not outperforms.
        </p>
      </td>
    </tr></tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>2022</heading>
            </td>
          </tr>
        </tbody></table>
	      
   <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
<!--           <div class="two" id='swis_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='assets/swis_image.jpg' width="180">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/9897562">
          <papertitle>SWIS: Self-Supervised Representation Learning for Writer Independent Offline Signature Verification</papertitle>
        </a>
        <br>
        <strong>Siladittya Manna</strong>,
        <a href="https://soumitri2001.github.io/">Soumitri Chattopadhyay</a>,
        <a href="https://saumikb.github.io/">Saumik Bhattacharya</a>,
        <a href="https://www.isical.ac.in/~umapada/">Umapada Pal</a>,  <br>
        <em>IEEE International Conference on Image Processing (ICIP)</em>, 2022 (Oral)
        <br>
        <a href="https://ieeexplore.ieee.org/abstract/document/9897562/"><span><i class="fas fa-scroll"></i></span>Paper</a>
        /
	<a href="https://soumitri2001.github.io/assets/SWIS_PPT.pdf"><span><i class="fa-solid fa-presentation-screen"></i></span>Slides</a>
	/
        <a href="https://arxiv.org/abs/2202.13078"><span><i class="ai ai-arxiv"></i></span>arXiv</a>
        <p></p>
        <p>
        We use a decorrelation-based loss to learn decorrelated stroke features from signature images for writer-independent signature verification.
        </p>
      </td>
    </tr>

<tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
<!--           <div class="two" id='swis_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='assets/surds_image.JPG' width="180">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/9956442">
          <papertitle>SURDS: Self-Supervised Attention-guided Reconstruction and Dual Triplet Loss for Writer Independent Offline Signature Verification</papertitle>
        </a>
        <br>
        <a href="https://soumitri2001.github.io/">Soumitri Chattopadhyay</a>,
	<strong>Siladittya Manna</strong>,
        <a href="https://saumikb.github.io/">Saumik Bhattacharya</a>,
        <a href="https://www.isical.ac.in/~umapada/">Umapada Pal</a>,  <br>
        <em>26th International Conference on Pattern Recognition (ICPR)</em>, 2022 (Oral)
        <br>
        <a href="https://ieeexplore.ieee.org/abstract/document/9956442"><span><i class="fas fa-scroll"></i></span>Paper</a>
        /
	<a href="https://youtu.be/xc_EVy6O7js"><span><i class="fa-solid fa-video"></i></span>Video</a>
	/
        <a href="https://arxiv.org/abs/2201.10138"><span><i class="ai ai-arxiv"></i></span>arXiv</a>
        <p></p>
        <p>
        we use an image reconstruction network using an encoder-decoder architecture that is augmented by a 2D spatial attention mechanism using signature image patches to learn representation. Next, we use a dual-triplet loss based framework for writer independent signature verification tasks.
        </p>
      </td>
    </tr>

<tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
<!--           <div class="two" id='swis_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='assets/plsm.png' width="180">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/9913326">
          <papertitle>PLSM: A Parallelized Liquid State Machine for Unintentional Action Detection</papertitle>
        </a>
        <br>
        <strong>Siladittya Manna</strong>,
	      <a href="">Dipayan Das</a>,
        <a href="https://saumikb.github.io/">Saumik Bhattacharya</a>,
        <a href="https://www.isical.ac.in/~umapada/">Umapada Pal</a>,
	      <a href="https://scholar.google.nl/citations?user=niNYlmUAAAAJ&hl=en">Sukalpa Chanda</a>,<br>
        <em>IEEE Transactions on Emerging Topics in Computing</em> Volume 11, Issue 2, Pages 474-484
        <br>
        <a href="https://ieeexplore.ieee.org/abstract/document/9913326"><span><i class="fas fa-scroll"></i></span>Paper</a>
<!--         /
        <a href="https://arxiv.org/abs/2007.07761"><span><i class="ai ai-arxiv"></i></span>arXiv</a> -->
        <p></p>
        <p>
        We present a novel Parallelized LSM (PLSM) architecture that incorporates spatio-temporal read-out layer and semantic constraints on model output. To the best of our knowledge, such a formulation has been done for the first time in literature, and it offers a computationally lighter alternative to traditional deep-learning models. Additionally, we also present a comprehensive algorithm for the implementation of parallelizable SNNs and LSMs that are GPU-compatible. We implement the PLSM model to classify unintentional/accidental video clips using the Oops dataset. From the experimental results on detecting unintentional action in a video, it can be observed that our proposed model outperforms a self-supervised model and a fully supervised traditional deep learning model.
        </p>
      </td>
    </tr>
	   
<tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
<!--           <div class="two" id='swis_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='assets/prl1_image.png' width="180">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://www.sciencedirect.com/science/article/pii/S0167865522000149">
          <papertitle>Self-supervised representation learning for detection of ACL tear injury in knee MR videos</papertitle>
        </a>
        <br>
        <strong>Siladittya Manna</strong>,
        <a href="https://saumikb.github.io/">Saumik Bhattacharya</a>,
        <a href="https://www.isical.ac.in/~umapada/">Umapada Pal</a>,  <br>
        <em>Pattern Recognition Letters</em> Volume 154, Pages 37-43
        <br>
        <a href="https://www.sciencedirect.com/science/article/pii/S0167865522000149"><span><i class="fas fa-scroll"></i></span>Paper</a>
        /
        <a href="https://arxiv.org/abs/2007.07761"><span><i class="ai ai-arxiv"></i></span>arXiv</a>
        <p></p>
        <p>
        we propose a self-supervised learning approach to learn transferable features from MR video clips by enforcing the model to learn anatomical features. The pretext task models are designed to predict the correct ordering of the jumbled image patches that the MR video frames are divided into. To the best of our knowledge, none of the supervised learning models performing injury classification task from MR video provide any explanation for the decisions made by the models and hence makes our work the first of its kind on MR video data.
        </p>
      </td>
    </tr>
</tbody></table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>2021</heading>
            </td>
          </tr>
        </tbody></table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
<!--           <div class="two" id='swis_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='assets/icvgip1.JPG' width="180">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/9956442">
          <papertitle>Interpretive self-supervised pre-training: boosting performance on visual medical data</papertitle>
        </a>
        <br>
	<strong>Siladittya Manna</strong>,
        <a href="https://saumikb.github.io/">Saumik Bhattacharya</a>,
        <a href="https://www.isical.ac.in/~umapada/">Umapada Pal</a>,  <br>
        <em>Proceedings of the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP)</em>, 2021 (Oral)
        <br>
        <a href="https://dl.acm.org/doi/abs/10.1145/3490035.3490273"><span><i class="fas fa-scroll"></i></span>Paper</a>
        <p></p>
        <p>
        In this work, we have proposed a novel loss function and derive it's asymptotic lower bound. We have also shown that self-supervised pre-training with the proposed loss function helps in surpassing the supervised baseline on the downstream task. We have also shown that the self-supervised pre-training helps a model in learning better representation in general to achieve better performance compared to supervised baselines. We have mathematically derived that the contrastive loss function asymptotically treats each sample as a separate class and works by maximizing the distance between any two samples and this helps to get better performance. Finally, through exhaustive experiments, we demonstrate that self-supervised pre-training helps to surpass the performance of fully supervised models on downstream tasks.
        </p>
      </td>
    </tr>
		
        </tbody></table>

<!-- 	<hr>
	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Co-Authors</heading>
            </td>
          </tr>
        </tbody></table>
	<table width="100%" align="center" border="0" cellpadding="20"><tbody>				
	<tr>
            <td>
	    <a href="">Saumik Bhattacharya</a><br>
	    <p></p>
            </td>
          </tr>
	</tbody></table> -->
	
	<hr>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Miscellany</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>				
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="assets/mida2023.jpeg" width="180"></td>
            <td width="75%" valign="center">
	    <a href="https://www.mida.org.in/">Hands-On Tutorial Session, MIDA 2023</a><br>
	    <p>Jointly Organized by Dept. of IT & CFSD, SMIT, Sikkim and IDEAS-TIH, ISI, Kolkata</p>              
<!-- 			  <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
              <br>
              <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member, CVPR 2021</a>
              <br>
              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
              <br>
              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a> -->
            </td>
          </tr>
          <!-- <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cs188.jpg" alt="cs188">
            </td>
            <td width="75%" valign="center">
              <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
              <br>
              <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
              <br>
              <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
            </td>
          </tr> -->
					

          <tr>
            <td align="center" style="padding:20px;width:25%;vertical-align:middle">
							<heading>Blog Posts</heading> <br>
		    <a href="https://medium.com/@mannasiladittya"><span><img src="assets/medium.svg" alt="Icon" class="icon"></span></a> &nbsp &nbsp
		    <a href="https://medium.com/the-owl"><span><img src="assets/theowl.png" alt="Icon" class="icon"></span></a>
		    					
            </td>
            <td width="75%" valign="middle">
              <a href="https://medium.com/the-owl/how-to-get-model-summary-in-pytorch-57db7824d1e3">How to get Model Summary in PyTorch</a>
              <br>
              <a href="https://medium.com/the-owl/how-to-plot-multiple-2d-series-in-3d-in-matplotlib-16f2d0aacd12">How to plot multiple 2D Series in 3D (Waterfall plot) in Matplotlib</a>
              <br>
              <a href="https://medium.com/the-owl/creating-a-tf-dataset-using-a-data-generator-5e5564609e64">Creating a TF Dataset using a Data Generator</a>
	    <br>
              <a href="https://medium.com/the-owl/using-forward-hooks-to-extract-intermediate-layer-outputs-from-a-pre-trained-model-in-pytorch-1ec17af78712">Using forward_hooks to Extract Intermediate Layer Outputs from a Pre-trained ResNet Model in PyTorch</a>
	    <br>
		<a href="https://medium.com/the-owl/plotting-grouped-bar-chart-in-matplotlib-7700e818344f">Plotting Grouped Bar Chart in Matplotlib</a>
            </td>
          </tr>
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
