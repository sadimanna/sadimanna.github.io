<!DOCTYPE HTML>
<html lang="en"><head>
      <meta name="google-site-verification" content="mzyi5h_x1DvmnvvOynztP5h3mA2mdBpUjfhySMB9NNI" />
      <!-- Google Tag Manager -->
      <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-NS48J6H');</script>
      <!-- End Google Tag Manager -->

      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <title>Siladittya Manna</title>
      <meta name="author" content="Siladittya Manna">
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <link rel="stylesheet" type="text/css" href="style.css">
      <link rel="stylesheet" type="text/css" href="stylesheet.css">
      <!-- <link rel="stylesheet" href="fontawesome.all.min.css"> -->
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
<!--       <link rel="icon" type="image/png" href="images/icon.png"> -->
      <link rel="icon" href="assets/profile_circle.png">
      <!-- <script defer src="./static/js/fontawesome.all.min.js"></script> -->


      <!-- Global site tag (gtag.js) - Google Analytics -->
      <!-- Need to change ############## -->
      <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=GTM-NS48J6H"></script>
      <script>
         window.dataLayer = window.dataLayer || [];
         function gtag(){dataLayer.push(arguments);}
         gtag('js', new Date());
         
         gtag('config', 'GTM-NS48J6H');
      </script> -->
      <!-- Need to change ############## -->
   </head>

<body>
  <table style="width:100%;max-width:1200px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Siladittya Manna</name>
              </p>
              <p>I am a Senior Research Assistant at the Department of Computer Science, Hong Kong Baptist University. My area of research includes Self-supervised learning, computer vision and medical image analysis.
              </p>
              <p style="text-align:center">
                &nbsp|&nbsp <a href="mailto:mannasiladittya@gmail.com"><img src="assets/envelope-solid.svg" alt="Icon" class="icon">Email</a> &nbsp|&nbsp <br> <!--&nbsp/&nbsp-->
                &nbsp|&nbsp <a href="assets/Siladittya Manna Curriculum Vitae.pdf"><img src="assets/file-solid.svg" alt="Icon" class="icon">CV</a> &nbsp|&nbsp
	      <a href="https://github.com/sadimanna/"><img src="assets/github.svg" alt="Icon" class="icon">Github</a> &nbsp|&nbsp <br>
                &nbsp|&nbsp <a href="https://scholar.google.com/citations?hl=en&user=6V9sqi0AAAAJ"><img src="assets/google-scholar.svg" alt="Icon" class="icon">Google Scholar</a> &nbsp|&nbsp
		<a href="https://dblp.org/pid/270/2011.html"><img src="assets/dblp.svg" alt="Icon" class="icon">dblp</a> &nbsp|&nbsp
		<a href="https://orcid.org/0000-0001-6364-8654"><img src="assets/orcid.svg" alt="Icon" class="icon">Orcid</a> &nbsp|&nbsp <br>
<!-- 		&nbsp|&nbsp <a href="https://in.pinterest.com/siladittya/"><img src="assets/pinterest.svg" alt="Icon" class="icon">Pinterest</a> &nbsp | &nbsp -->
	      &nbsp|&nbsp <a href="https://twitter.com/sadimanna"><img src="assets/twitter.svg" alt="Icon" class="icon">Twitter</a> &nbsp|&nbsp
		<a href="https://medium.com/@mannasiladittya"><img src="assets/medium (1).svg" alt="Icon" class="icon">Medium</a> &nbsp|&nbsp <br>
<!-- 		&nbsp|&nbsp <a href="https://twitter.com/sadimanna"><img src="assets/twitter.svg" alt="Icon" class="icon">Twitter</a> &nbsp|&nbsp -->
              </p>
	    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:2.5%;height:100px;vertical-align:middle;text-align:center">
              <a href="assets/iiestlogo.png"><img style="height:150px" alt="iiests photo" src="assets/iiestlogo.png" class="hoverZoomLink"></a>
            </td>
<!-- 	    <td style="padding:2.5%;vertical-align:middle;text-align:center">
		    <div class="arrow">
		        <span></span>
		        <span></span>
		        <span></span>
		    </div>
	    </td> -->
	    <td style="padding:2.5%;height:100px;vertical-align:middle;text-align:center">
              <a href="assets/isicallogo.png"><img style="height:150px" alt="isical photo" src="assets/isicallogo.png" class="hoverZoomLink"></a>
            </td>
		<td style="padding:2.5%;height:100px;vertical-align:middle;text-align:center">
              <a href="assets/isicallogo.png"><img style="height:150px" alt="isical photo" src="assets/HKBU_Logo.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="assets/profilepic.png"><img style="width:100%;max-width:100%" alt="profile photo" src="assets/profilepic.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Bio</heading>
              <p>
                I am working as a Senior Research Assistant at the Department of Computer Science, Hong Kong Baptist University, under the supervision of <a href="https://www.comp.hkbu.edu.hk/~ymc/">Prof. Yiu-ming Cheung</a>. Previously, I was pursuing my PhD at the Indian Statistical Institute, Kolkata, where I am advised by <a href="https://www.isical.ac.in/~umapada/">Prof. Umapada Pal</a> and <a href="https://www.iitkgp.ac.in/department/EC/faculty/ec-saumik">Dr. Saumik Bhattacharya</a>. Before joining ISI, Kolkata, I did my B.Tech and M.Tech (Dual Degree) from Indian Institute of Engineering Science and Technology, Shibpur, Howrah, where I was advised by <a href="https://www.iiests.ac.in/IIEST/Faculty/telecom-ankita">Dr. Ankita Pramanik</a> for my Master's Thesis.
              </p>
            </td>
          </tr>
        </tbody></table>
	
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="padding:20px;width:100%;vertical-align:middle">
                              <heading>
                                 <font size="5">Recent Updates </font>
                              </heading>
                              <p>
				<li>
                                 <a class="button" style="color:#FFFFFF"><strong style="font-size:12px"> New! </strong></a>
                                 <em style="font-size:14px;"><b>[December 2024:]</b> <a href=""><strong>One paper</strong></a> accepted to <strong>IEEE Transactions on Aritifical Intelligence</strong>.</em>
                              </li>
				<li>
                                 <a class="button" style="color:#FFFFFF"><strong style="font-size:12px"> New! </strong></a>
                                 <em style="font-size:14px;"><b>[August 2024:]</b> <a href=""><strong>One paper</strong></a> accepted to <strong>ICPR 2024</strong>.</em>
                              </li>
                              <li>
                                 <a class="button" style="color:#FFFFFF"><strong style="font-size:12px"> New! </strong></a>
                                 <em style="font-size:14px;"><b>[August 2024:]</b> <a href="https://openreview.net/forum?id=3Wg1oErMcJ"><strong>One survey paper</a></strong> accepted to <strong>Transactions on Machine Learning Research</strong>.</em>
                              </li>
<!--                               <li>
                                 <em style="font-size:14px;"><b>[Dec 2017:]</b> <a href="https://ieeexplore.ieee.org/abstract/document/8593100/"> One paper</a> accepted in <a href="https://www.isical.ac.in/~icapr17/">ICAPR 2017</a>.</em>
                              </li> -->
                              </p>
                           </td>
                        </tr>
                     </tbody>
        </table>
		
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>2024</heading>
            </td>
          </tr>
        </tbody></table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
<!--           <div class="two" id='swis_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='assets/supp_fig2.png' width="180">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2308.01140">
          <papertitle>Dynamically Scaled Temperature in Self-Supervised Contrastive Learning</papertitle>
        </a>
        <br>
        <strong>Siladittya Manna</strong>,
	      <a href="https://soumitri2001.github.io/">Soumitri Chattopadhyay*</a>,
	      <a href="https://scholar.google.com/citations?hl=en&user=sz1Hu3gAAAAJ">Rakesh Dey*</a>,
	      <a href="https://saumikb.github.io/">Saumik Bhattacharya</a>,
        <a href="https://www.isical.ac.in/~umapada/">Umapada Pal</a>,<br>
        <em>Accepted to IEEE Transactions on Artificial Intelligence</em>
        <br>
<!--         <a href="https://ieeexplore.ieee.org/abstract/document/9897562/"><span><i class="fas fa-scroll"></i></span>Paper</a>
        / -->
<!-- 	<a href="https://soumitri2001.github.io/assets/SWIS_PPT.pdf"><span><i class="fa-solid fa-presentation-screen"></i></span>Slides</a>
	/ -->
        <a href="https://arxiv.org/abs/2308.01140"><span><i class="ai ai-arxiv"></i></span>arXiv</a>
        <p></p>
        <p>
       We focus our attention on improving the performance of InfoNCE loss in SSL by studying the effect of temperature 
		hyper-parameter values. We propose a cosine similarity-dependent temperature scaling function to effectively optimize the 
		distribution of the samples in the feature space after rigorous mathematical analysis of the gradient of the InfoNCE loss 
		with respect to the pair-wise cosine similarity of the sample pairs. We further analyze the uniformity and tolerance metrics to investigate 
		the optimal regions in the cosine similarity space for better optimization. Additionally, we offer a comprehensive examination 
		of the behaviour of local and global structures in the feature space throughout the pre-training phase, as the temperature varies.
        </p>
      </td>
    </tr>
	
<tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
<!--           <div class="two" id='swis_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='assets/surveypart1.png' width="180">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openreview.net/forum?id=3Wg1oErMcJ">
          <papertitle>Self-Supervised Visual Representation Learning for Medical Image Analysis: A Comprehensive Survey</papertitle>
        </a>
        <br>
        <strong>Siladittya Manna</strong>,
	      <a href="https://saumikb.github.io/">Saumik Bhattacharya</a>,
        <a href="https://www.isical.ac.in/~umapada/">Umapada Pal</a>,<br>
        <em>Transactions on Machine Learning Research (2024)</em>
        <br>
        <a href="https://openreview.net/forum?id=3Wg1oErMcJ"><span><i class="fas fa-scroll"></i></span>Paper</a>
<!-- 	<a href="https://soumitri2001.github.io/assets/SWIS_PPT.pdf"><span><i class="fa-solid fa-presentation-screen"></i></span>Slides</a>
	/ -->
<!--         <a href="https://arxiv.org/abs/2308.01140"><span><i class="ai ai-arxiv"></i></span>arXiv</a> -->
        <p></p>
        <p>
       In this study, we attempt to present a review of those methods and show how the self-supervised learning paradigm evolved over the years. 
	Additionally, we also present an exhaustive review of the self-supervised methods applied to medical image analysis. 
	Furthermore, we also present an extensive compilation of the details of the datasets used in the different works and provide performance metrics of some notable works on image and video datasets.
        </p>
      </td>
    </tr>

	
<tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
<!--           <div class="two" id='swis_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='assets/cowproe2e.png' width="180">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2408.06235">
          <papertitle>Correlation Weighted Prototype-based Self-Supervised One-Shot Segmentation of Medical Images</papertitle>
        </a>
        <br>
        <strong>Siladittya Manna</strong>,
	      <a href="https://saumikb.github.io/">Saumik Bhattacharya</a>,
        <a href="https://www.isical.ac.in/~umapada/">Umapada Pal</a>,<br>
        <em>27th International Conference on Pattern Recognition</em> (2024)
        <be>
	<a href="https://link.springer.com/chapter/10.1007/978-3-031-78192-6_2"><span><i class="fas fa-scroll"></i></span>Paper</a>
        /
        <a href="https://arxiv.org/abs/2408.06235"><span><i class="ai ai-arxiv"></i></span>arXiv</a>
        <p></p>
        <p>
       In this work, we adopt a prototype-based self-supervised one-way one-shot learning framework using pseudo-labels generated from superpixels to learn the semantic segmentation task itself. 
		We use a correlation-based probability score to generate a dynamic prototype for each query pixel from the bag of prototypes obtained from the support feature map. 
		This weighting scheme helps to give a higher weightage to contextually related prototypes. 
        </p>
      </td>
    </tr>
</tbody></table>
        
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>2023</heading>
            </td>
          </tr>
        </tbody></table>
        
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

   <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
<!--           <div class="two" id='swis_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='assets/skid_image.png' width="180">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://doi.org/10.1109/TAI.2023.3299883">
          <papertitle>Self-Supervised Representation Learning for Knee Injury Diagnosis from Magnetic Resonance Data</papertitle>
        </a>
        <br>
        <strong>Siladittya Manna</strong>,
        <a href="https://saumikb.github.io/">Saumik Bhattacharya</a>,
        <a href="https://www.isical.ac.in/~umapada/">Umapada Pal</a>,  <br>
        <em>IEEE Transactions on Artificial Intelligence</em>
        <br>
        <a href="https://ieeexplore.ieee.org/abstract/document/10197577"><span><i class="fas fa-scroll"></i></span>Paper</a>
        /
	<a href="https://github.com/sadimanna/skid"><span><i class="fa-solid fa-code"></i></span>Code</a>
	/
        <a href="https://arxiv.org/abs/2104.10481"><span><i class="ai ai-arxiv"></i></span>arXiv</a>
        <p></p>
        <p>
        We propose a self-supervised learning (SSL) approach for learning the spatial anatomical representations from the frames of magnetic resonance (MR) video clips for the diagnosis of knee medical conditions. The pretext model learns meaningful, context-invariant spatial representations. The downstream task in our paper is a class-imbalanced multi-label classification. To the best of our knowledge, this work is the first of its kind in showing the effectiveness and reliability of self-supervised learning algorithms in imbalanced multi-label classification tasks on MR scans.
        </p>
      </td>
    </tr>
  
<tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
<!--           <div class="two" id='swis_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='assets/mio_image2.png' width="180">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2111.12664">
          <papertitle>MIO: Mutual Information Optimization using Self-Supervised Binary Contrastive Learning</papertitle>
        </a>
        <br>
        <strong>Siladittya Manna</strong>,
        <a href="https://www.isical.ac.in/~umapada/">Umapada Pal</a>,
	      <a href="https://saumikb.github.io/">Saumik Bhattacharya</a>,<br>
        <em>Under Review</em>
        <br>
<!--         <a href="https://ieeexplore.ieee.org/abstract/document/9897562/"><span><i class="fas fa-scroll"></i></span>Paper</a>
        / -->
<!-- 	<a href="https://soumitri2001.github.io/assets/SWIS_PPT.pdf"><span><i class="fa-solid fa-presentation-screen"></i></span>Slides</a>
	/ -->
        <a href="https://arxiv.org/abs/2111.12664"><span><i class="ai ai-arxiv"></i></span>arXiv</a>
        <p></p>
        <p>
        we propose a novel loss function for contrastive learning. We model our pre-training task as a binary classification problem to induce an implicit contrastive effect. We further improve the n\"aive loss function after removing the effect of the positive-positive repulsion and incorporating the upper bound of the negative pair repulsion. Unlike existing methods, the proposed loss function optimizes the mutual information in both positive and negative pairs. We also present a closed-form expression for the parameter gradient flow and compare the behavior of self-supervised contrastive frameworks using Hessian eigenspectrum to analytically study their convergence. The proposed method outperforms SOTA self-supervised contrastive frameworks on benchmark datasets such as CIFAR-10, CIFAR-100, STL-10, and Tiny-ImageNet. After 200 pretraining epochs with ResNet-18 as the backbone, the proposed model achieves an accuracy of 86.36%, 58.18%, 80.50%, and 30.87% on the CIFAR-10, CIFAR-100, STL-10, and Tiny-ImageNet datasets, respectively, and surpasses the SOTA contrastive baseline by 1.93\%, 3.57\%, 4.85\%, and 0.33\%, respectively. The proposed framework also achieves a state-of-the-art accuracy of 78.4% (200 epochs) and 65.22% (100 epochs) Top-1 Linear Evaluation accuracy on ImageNet100 and ImageNet1K datasets, respectively.
        </p>
      </td>
    </tr>
		
   <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
<!--           <div class="two" id='swis_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='assets/selfdocseg.png' width="180">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2305.00795">
          <papertitle>SelfDocSeg: A Self-Supervised vision-based Approach towards Document Segmentation</papertitle>
        </a>
        <br>
	      <a href="">Subhajit Maity</a>,
	      <a href="">Sanket Biswas</a>,
        <strong>Siladittya Manna</strong>,
        <a href="">Ayan Banerjee</a>,
	      <a href="">Joseph Llados</a>,
        <a href="https://saumikb.github.io/">Saumik Bhattacharya</a>,
        <a href="https://www.isical.ac.in/~umapada/">Umapada Pal</a>,  <br>
        <em>18th International Conference on Document Analysis and Recognition (ICDAR)</em>, 2024 (Oral)
        <br>
        <a href="https://link.springer.com/chapter/10.1007/978-3-031-41676-7_20"><span><i class="fas fa-scroll"></i></span>Paper</a>
        /
	<a href="https://github.com/MaitySubhajit/SelfDocSeg"><span><i class="fa-solid fa-code"></i></span>Code</a>
        <p></p>
        <p>
        With growing internet connectivity to personal life, an enormous amount of documents had been available in the public domain and thus making data annotation a tedious task. We address this challenge using self-supervision and unlike, the few existing self-supervised document segmentation approaches which use text mining and textual labels, we use a complete vision-based approach in pre-training without any ground-truth label or its derivative. Instead, we generate pseudo-layouts from the document images to pre-train an image encoder to learn the document object representation and localization in a self-supervised framework before fine-tuning it with an object detection model. We show that our pipeline sets a new benchmark in this context and performs at par with the existing methods and the supervised counterparts, if not outperforms.
        </p>
      </td>
    </tr></tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>2022</heading>
            </td>
          </tr>
        </tbody></table>
	      
   <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
<!--           <div class="two" id='swis_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='assets/swis_image.jpg' width="180">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/9897562">
          <papertitle>SWIS: Self-Supervised Representation Learning for Writer Independent Offline Signature Verification</papertitle>
        </a>
        <br>
        <strong>Siladittya Manna</strong>,
        <a href="https://soumitri2001.github.io/">Soumitri Chattopadhyay</a>,
        <a href="https://saumikb.github.io/">Saumik Bhattacharya</a>,
        <a href="https://www.isical.ac.in/~umapada/">Umapada Pal</a>,  <br>
        <em>IEEE International Conference on Image Processing (ICIP)</em>, 2022 (Oral)
        <br>
        <a href="https://ieeexplore.ieee.org/abstract/document/9897562/"><span><i class="fas fa-scroll"></i></span>Paper</a>
        /
	<a href="https://soumitri2001.github.io/assets/SWIS_PPT.pdf"><span><i class="fa-solid fa-presentation-screen"></i></span>Slides</a>
	/
        <a href="https://arxiv.org/abs/2202.13078"><span><i class="ai ai-arxiv"></i></span>arXiv</a>
        <p></p>
        <p>
        We use a decorrelation-based loss to learn decorrelated stroke features from signature images for writer-independent signature verification.
        </p>
      </td>
    </tr>

<tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
<!--           <div class="two" id='swis_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='assets/surds_image.JPG' width="180">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/9956442">
          <papertitle>SURDS: Self-Supervised Attention-guided Reconstruction and Dual Triplet Loss for Writer Independent Offline Signature Verification</papertitle>
        </a>
        <br>
        <a href="https://soumitri2001.github.io/">Soumitri Chattopadhyay</a>,
	<strong>Siladittya Manna</strong>,
        <a href="https://saumikb.github.io/">Saumik Bhattacharya</a>,
        <a href="https://www.isical.ac.in/~umapada/">Umapada Pal</a>,  <br>
        <em>26th International Conference on Pattern Recognition (ICPR)</em>, 2022 (Oral)
        <br>
        <a href="https://ieeexplore.ieee.org/abstract/document/9956442"><span><i class="fas fa-scroll"></i></span>Paper</a>
        /
	<a href="https://youtu.be/xc_EVy6O7js"><span><i class="fa-solid fa-video"></i></span>Video</a>
	/
        <a href="https://arxiv.org/abs/2201.10138"><span><i class="ai ai-arxiv"></i></span>arXiv</a>
        <p></p>
        <p>
        we use an image reconstruction network using an encoder-decoder architecture that is augmented by a 2D spatial attention mechanism using signature image patches to learn representation. Next, we use a dual-triplet loss based framework for writer independent signature verification tasks.
        </p>
      </td>
    </tr>

<tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
<!--           <div class="two" id='swis_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='assets/plsm.png' width="180">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/9913326">
          <papertitle>PLSM: A Parallelized Liquid State Machine for Unintentional Action Detection</papertitle>
        </a>
        <br>
        <strong>Siladittya Manna</strong>,
	      <a href="">Dipayan Das</a>,
        <a href="https://saumikb.github.io/">Saumik Bhattacharya</a>,
        <a href="https://www.isical.ac.in/~umapada/">Umapada Pal</a>,
	      <a href="https://scholar.google.nl/citations?user=niNYlmUAAAAJ&hl=en">Sukalpa Chanda</a>,<br>
        <em>IEEE Transactions on Emerging Topics in Computing</em> Volume 11, Issue 2, Pages 474-484
        <br>
        <a href="https://ieeexplore.ieee.org/abstract/document/9913326"><span><i class="fas fa-scroll"></i></span>Paper</a>
<!--         /
        <a href="https://arxiv.org/abs/2007.07761"><span><i class="ai ai-arxiv"></i></span>arXiv</a> -->
        <p></p>
        <p>
        We present a novel Parallelized LSM (PLSM) architecture that incorporates spatio-temporal read-out layer and semantic constraints on model output. To the best of our knowledge, such a formulation has been done for the first time in literature, and it offers a computationally lighter alternative to traditional deep-learning models. Additionally, we also present a comprehensive algorithm for the implementation of parallelizable SNNs and LSMs that are GPU-compatible. We implement the PLSM model to classify unintentional/accidental video clips using the Oops dataset. From the experimental results on detecting unintentional action in a video, it can be observed that our proposed model outperforms a self-supervised model and a fully supervised traditional deep learning model.
        </p>
      </td>
    </tr>
	   
<tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
<!--           <div class="two" id='swis_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='assets/prl1_image.png' width="180">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://www.sciencedirect.com/science/article/pii/S0167865522000149">
          <papertitle>Self-supervised representation learning for detection of ACL tear injury in knee MR videos</papertitle>
        </a>
        <br>
        <strong>Siladittya Manna</strong>,
        <a href="https://saumikb.github.io/">Saumik Bhattacharya</a>,
        <a href="https://www.isical.ac.in/~umapada/">Umapada Pal</a>,  <br>
        <em>Pattern Recognition Letters</em> Volume 154, Pages 37-43
        <br>
        <a href="https://www.sciencedirect.com/science/article/pii/S0167865522000149"><span><i class="fas fa-scroll"></i></span>Paper</a>
        /
        <a href="https://arxiv.org/abs/2007.07761"><span><i class="ai ai-arxiv"></i></span>arXiv</a>
        <p></p>
        <p>
        we propose a self-supervised learning approach to learn transferable features from MR video clips by enforcing the model to learn anatomical features. The pretext task models are designed to predict the correct ordering of the jumbled image patches that the MR video frames are divided into. To the best of our knowledge, none of the supervised learning models performing injury classification task from MR video provide any explanation for the decisions made by the models and hence makes our work the first of its kind on MR video data.
        </p>
      </td>
    </tr>
</tbody></table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>2021</heading>
            </td>
          </tr>
        </tbody></table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
<!--           <div class="two" id='swis_image'><video  width=100% height=100% muted autoplay loop>
          <source src="assets/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='assets/icvgip1.JPG' width="180">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/9956442">
          <papertitle>Interpretive self-supervised pre-training: boosting performance on visual medical data</papertitle>
        </a>
        <br>
	<strong>Siladittya Manna</strong>,
        <a href="https://saumikb.github.io/">Saumik Bhattacharya</a>,
        <a href="https://www.isical.ac.in/~umapada/">Umapada Pal</a>,  <br>
        <em>Proceedings of the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP)</em>, 2021 (Oral)
        <br>
        <a href="https://dl.acm.org/doi/abs/10.1145/3490035.3490273"><span><i class="fas fa-scroll"></i></span>Paper</a>
        <p></p>
        <p>
        In this work, we have proposed a novel loss function and derive it's asymptotic lower bound. We have also shown that self-supervised pre-training with the proposed loss function helps in surpassing the supervised baseline on the downstream task. We have also shown that the self-supervised pre-training helps a model in learning better representation in general to achieve better performance compared to supervised baselines. We have mathematically derived that the contrastive loss function asymptotically treats each sample as a separate class and works by maximizing the distance between any two samples and this helps to get better performance. Finally, through exhaustive experiments, we demonstrate that self-supervised pre-training helps to surpass the performance of fully supervised models on downstream tasks.
        </p>
      </td>
    </tr>
		
        </tbody></table>

<!-- 	<hr>
	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Co-Authors</heading>
            </td>
          </tr>
        </tbody></table>
	<table width="100%" align="center" border="0" cellpadding="20"><tbody>				
	<tr>
            <td>
	    <a href="">Saumik Bhattacharya</a><br>
	    <p></p>
            </td>
          </tr>
	</tbody></table> -->
	
	<hr>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Miscellany</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>				
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="assets/mida2023.jpeg" width="180"></td>
            <td width="75%" valign="center">
	    <a href="https://www.mida.org.in/">Hands-On Tutorial Session, MIDA 2023</a><br>
	    <p>Jointly Organized by Dept. of IT & CFSD, SMIT, Sikkim and IDEAS-TIH, ISI, Kolkata</p>              
<!-- 			  <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
              <br>
              <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member, CVPR 2021</a>
              <br>
              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
              <br>
              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a> -->
            </td>
          </tr>
          <!-- <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cs188.jpg" alt="cs188">
            </td>
            <td width="75%" valign="center">
              <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
              <br>
              <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
              <br>
              <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
            </td>
          </tr> -->
					

          <tr>
            <td align="center" style="padding:20px;width:25%;vertical-align:middle">
							<heading>Blog Posts</heading> <br>
		    <a href="https://medium.com/@mannasiladittya"><span><img src="assets/medium.svg" alt="Icon" class="icon"></span></a> &nbsp &nbsp
		    <a href="https://medium.com/the-owl"><span><img src="assets/theowl.png" alt="Icon" class="icon"></span></a>
		    					
            </td>
            <td width="75%" valign="middle">
		    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
			    <tr><td>Blog Title</td><td>Views</td></tr>
		    <tr><td><a href="https://medium.com/the-owl/train-fasterrcnn-faster-with-16-bit-precision-in-detectron2-a2d644b91ef5">Train FasterRCNN faster with 16-bit precision in Detectron2</a></td><td>389</td>
              </tr>
		   <tr><td> <a href="https://medium.com/the-owl/guide-to-fine-tuning-a-pre-trained-model-for-object-detection-tasks-with-faster-rcnn-using-e2e7e190105b">Guide to fine-tuning a Pre-trained model for Object Detection tasks with Faster RCNN using Detectron2</a></td><td>2.2K</td>
              </tr>
		  <!--  <a href="https://medium.com/the-owl/the-art-of-loading-custom-objects-and-functions-in-keras-6fdea8ed52b9">The Art of Loading Custom Objects and Functions in Keras</a>
              <br>
		    <a href="https://medium.com/the-owl/weighted-categorical-cross-entropy-loss-in-keras-edaee1df44ee">Weighted Categorical Cross-Entropy Loss in Keras</a>
              <br>
		    <a href="https://medium.com/the-owl/weighted-binary-cross-entropy-losses-in-keras-e3553e28b8db">Weighted Binary Cross-Entropy Loss in Keras</a>
              <br>
		    <a href="https://medium.com/the-owl/install-detectron2-73d9447d3652">Installing detectron2 in 9 easy steps</a>
              <br>
              <a href="https://medium.com/the-owl/how-to-get-model-summary-in-pytorch-57db7824d1e3">How to get Model Summary in PyTorch</a>
              <br>
              <a href="https://medium.com/the-owl/how-to-plot-multiple-2d-series-in-3d-in-matplotlib-16f2d0aacd12">How to plot multiple 2D Series in 3D (Waterfall plot) in Matplotlib</a>
              <br>
              <a href="https://medium.com/the-owl/creating-a-tf-dataset-using-a-data-generator-5e5564609e64">Creating a TF Dataset using a Data Generator</a>
	    <br>
              <a href="https://medium.com/the-owl/using-forward-hooks-to-extract-intermediate-layer-outputs-from-a-pre-trained-model-in-pytorch-1ec17af78712">Using forward_hooks to Extract Intermediate Layer Outputs from a Pre-trained ResNet Model in PyTorch</a>
	    <br>
		<a href="https://medium.com/the-owl/plotting-grouped-bar-chart-in-matplotlib-7700e818344f">Plotting Grouped Bar Chart in Matplotlib</a>
			  -->
		    </tbody></table>
            </td>
          </tr>
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
